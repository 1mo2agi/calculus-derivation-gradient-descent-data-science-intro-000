{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in past had talked about derivatives to understand speed and direction with which to move \n",
    "Remember that we are attempting to use this information to help us understand the path to an optimal line.  As of yet it still may be a bit hazy as to how exactly these derivatives point us to an optimal function.  Welll, hat we hope to show in this lesson, is that the partial derivatives will help us to find the tangent to the graph of a multivariable function, and that this tangent line is our path to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to move in the direction that will bring us the greatest change, there is a little bit of magic that occurs.  At any given point, to find the direction of greatest ascent we need to move some distance along the x-axis and some distance along the y axis.  \n",
    "\n",
    "The distance along the x-axis to move, ends up being the partial derivative of x.  The distance along the y-axis to move is the distance along the partial derivative of y.  \n",
    "\n",
    "In other words, if we have an equation $f(x,y) = 2x + 3y $, then the partial derivatives of the function are: \n",
    "\n",
    "$df/dx = 2 $ and $ df/dy = 3 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what this tells us is that for the function above, to move in the direction of greatest ascent, we should continuet to move up three and to the right 2.  So according to this, we would expect our direction of greatest ascent to look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./DirectionGradientAscent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at a 3d graph of our function, $ f(x, y) = 2x + 3y $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./3dx3y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to map up pretty well.  You can also get a sense that if that is the way to increase the output of the function as fast as possibl is simply to move in the direction of the respective partial derivatives, then to decrease the value of the function most rapidly, we should move in the opposite direction.  In this case, that direction would be in the direction x = -3 and y = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the idea behind gradient descent.  By now you know that we can take the derivative of a single-variable function to calculate how a small change in the input will change the output of the function.  And we know that the partial derivative allows us to determine how a change along a specific dimension or axis will effect our output.  The gradient is the partial derivative with respect to each type of variable, in this case x and y.  And the import of the gradient is that it's direction is the direction of steepest ascent.  The negative gradient, that is the negative of each of the partial derivatives, is the direction of steepest descent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Gradient Descent to our Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so why did we talk about this again?  Well if we can see how shifts in changes in our input will change our output, as derivatives allow us to do.  Well, then we can apply this to our cost function for our regression line.  That is, we can see how changes in our slope value and y-intercept value changes our cost.  So given a random regression line, we can figure which direction to move next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so our cost function looks like the following: $ error^2 = (guess - actual)^2 $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or make this look more like math, and put this in terms of our y values and our regression function, it looks like the following: \n",
    "\n",
    "$ \\epsilon^2 = (mx + b - y(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's frame this as equal to  J(m, b) to represent that this is a function that changes with those variables.\n",
    "\n",
    "$J(m, b) = (mx + b - y(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so remember we want to find the values of $m$ and $b$ that minimize our $\\epsilon^2 $, and that gradient descent how to change variables to move in the direction of a minimum output.  Essentially, we need to take the gradient of this function - meaning that we need to find $dJ/dm $ and  $dJ/db $.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ dJ/dm = 2(mx+ b - y)^1*x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule: $ y = x^2 $ and $ x = z $, to take dy/dz its dy/dx*dx/dz.  So find how to use derivatives with this exponents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.brightstorm.com/math/calculus/techniques-of-differentiation/chain-rule-the-general-exponential-rule/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
