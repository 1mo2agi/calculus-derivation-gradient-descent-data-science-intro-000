{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "As you know, we entered our discussion of derivatives to understand speed and direction with which to move along a cost curve.  This led to learn about finding a derivative in a single variable function and then in a multivariable function with partial derivatives.  However, we have not yet explicitly showed how partial derivatives apply to gradient descent.\n",
    "\n",
    "Well, that's what we hope to show in this lesson: how partial derivatives the path to minimize our cost function, and thus find our \"best fit\" regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the steepest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now gradient descent literally means that we are taking the shortest path to *descend* towards our minimum.  However, it is somewhat easier to understand gradient ascent than descent, and the two are quite related, so that's where we'll begin.  Gradient ascent simply means that we want to move in the direction of steepest ascent.\n",
    "\n",
    "Now moving in the direction of greatest ascent for a function $f(x,y)$, means that our next step is a step some distance in the $x$ direction and some distance in the $y$ direction that is the steepest upward at that point.\n",
    "\n",
    "![](./Denali.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this is a different task from what we have previously worked on for multivariable functions.   So far, we have used partial derivatives to calculate the **gain** from moving directly in either the $x$ direction or the $y$ direction.  Here in finding gradient ascent, our task is not to calculate the gain from a move in either the $x$ or $y$ direction.  Instead our task is to find some combination of a change in x,y that brings the largest change in output.  \n",
    "\n",
    "In finding that direction of largest change, we do use partial derivatives.  As we know, the partial derivative $\\frac{df}{dx}$ calculates the change in output from moving a little bit in the $x$ direction, and the partial derivative $\\frac{df}{dy}$ calculates the change in output from moving in the $y$ direction.  Because moving in the direction of steepest ascent is the change in x, y that produces the greatest change in output, if $\\frac{df}{dy} > \\frac{df}{dx}$, we should move more in the $y$ direction than the $x$ direction, and vice versa.  That is, we want to get the biggest bang for our buck.    \n",
    "\n",
    "In fact, the direction of greatest ascent for a function $f(x, y)$ is the step where me move a proportion of $\\frac{df}{dy}$ steps in the $y$ direction and $\\frac{df}{dx}$ steps in the $x$ direction.  So if $\\frac{df}{dy}$ = 5 and $\\frac{df}{dx}$ = 1, our next step she be five times more in the $y$ direction than the $x$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this in an example.  Here is a plot of a function:\n",
    "$$f(x,y) = 2x + 3y $$\n",
    "\n",
    "![](./3dx3y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you imagine being at the bottom left of the graph at the point $x = 1$, $y = 1$, what would be the direction of steepest ascent?  It seems, just sizing it up visually, that we should move diagonally in the positive $y$ direction and positive $x$ direction.  Looking more carefully, it seems we should move more in the $y$ direction than the $x$ direction.  Let's see what our technique of taking the partial derivative indicates.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivatives of the function $f(x,y) = 2x + 3y $ are the following: \n",
    "\n",
    "$\\frac{df}{dx}(2x + 3y) = 2 $ and $\\frac{df}{dy}(2x + 3y) = 3 $.\n",
    "\n",
    "And what this tells us is that for the function above, to move in the direction of greatest ascent, we should move up three and to the right two.  So we would expect our path of greatest ascent to look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./DirectionGradientAscent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this path seems to map up pretty well to what we saw visually.  That is the idea behind gradient descent.  By now you know that we can take the derivative of a single-variable function to calculate how a small change in the input will change the output of the function.  And we know that the partial derivative allows us to determine how a change along a specific dimension or axis will effect our output.  The gradient is the partial derivative with respect to each type of variable, in this case x and y.  And the import of the gradient is that it's direction is the direction of steepest ascent.  The negative gradient, that is the negative of each of the partial derivatives, is the direction of steepest descent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Gradient Descent to our Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so why did we talk about this again?  Well if we can see how shifts in changes in our input will change our output, as derivatives allow us to do.  Well, then we can apply this to our cost function for our regression line.  That is, we can see how changes in our slope value and y-intercept value changes our cost.  So given a random regression line, we can figure which direction to move next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so our cost function looks like the following: $ error^2 = (guess - actual)^2 $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or make this look more like math, and put this in terms of our y values and our regression function, it looks like the following: \n",
    "\n",
    "$ \\epsilon^2 = (mx + b - y(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's frame this as equal to  J(m, b) to represent that this is a function that changes with those variables.\n",
    "\n",
    "$J(m, b) = (mx + b - y(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so remember we want to find the values of $m$ and $b$ that minimize our $\\epsilon^2 $, and that gradient descent how to change variables to move in the direction of a minimum output.  Essentially, we need to take the gradient of this function - meaning that we need to find $dJ/dm $ and  $dJ/db $.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ dJ/dm = 2(mx+ b - y)^1*x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule: $ y = x^2 $ and $ x = z $, to take dy/dz its dy/dx*dx/dz.  So find how to use derivatives with this exponents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.brightstorm.com/math/calculus/techniques-of-differentiation/chain-rule-the-general-exponential-rule/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
