{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "As you know, we entered our discussion of derivatives to determine size and direction of a step with which to move along a cost curve.  We first used a derivative in a single variable function to see how the output of our cost curve changed with respect to change a change in our regression line's y-intercept or slope.  Then we learned about partial derivatives to see how a three dimensional cost curve responded to a change in a regression line's y-intercept or slope.  \n",
    "\n",
    "However, we have not yet explicitly showed how partial derivatives apply to gradient descent.\n",
    "\n",
    "Well, that's what we hope to show in this lesson: explain how we can use partial derivatives to find the path to minimize our cost function, and thus find our \"best fit\" regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the steepest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now gradient descent literally means that we are taking the shortest path to *descend* towards our minimum.  However, it is somewhat easier to understand gradient ascent than descent, and the two are quite related, so that's where we'll begin.  Gradient ascent, as you could guess, simply means that we want to move in the direction of steepest ascent.\n",
    "\n",
    "Now moving in the direction of greatest ascent for a function $f(x,y)$, means that our next step is a step some distance in the $x$ direction and some distance in the $y$ direction that is the steepest upward at that point.\n",
    "\n",
    "![](./Denali.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this is a different task from what we have previously worked on for multivariable functions.   So far, we have used partial derivatives to calculate the **gain** from moving directly in either the $x$ direction or the $y$ direction.  Here in finding gradient ascent, our task is not to calculate the gain from a move in either the $x$ or $y$ direction.  Instead our task is to find some combination of a change in $x$,$y$ that brings the largest change in output.  So if you look at the path our climbers are taking in the picture above, that is the direction of gradient ascent.  If they tilt their path to the right or left, they will no longer be moving along the steepest upward path.\n",
    "\n",
    "The direction of the greatest rate of increase of a function is called the gradient.  We denote the gradient with the nabla, which comes from the Greek word for harp, which is kind of what it looks like: $\\nabla $.  So we can denote the gradient of a function, $f(x, y)$, with $\\nabla f(x, y) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we find the direction fo the greatest rate of increase?  We use partial derivatives.  Here's why.\n",
    "\n",
    "As we know, the partial derivative $\\frac{df}{dx}$ calculates the change in output from moving a little bit in the $x$ direction, and the partial derivative $\\frac{df}{dy}$ calculates the change in output from moving in the $y$ direction.  Because with gradient ascent our goal is to make a nudge in $x, y$ that produces the greatest change in output, if $\\frac{df}{dy} > \\frac{df}{dx}$, we should make that move more in the $y$ direction than the $x$ direction, and vice versa.  That is, we want to get the biggest bang for our buck.  \n",
    "\n",
    "Let's relate this again to the picture of our mountain climbers. Imagine the vertical edge on the left is our y-axis and the horizontal edge is on the bottom is our x-axis.  For the climber in the yellow jacket, imagine his step size is three feet. A step straight along the y-axis will move him further upwards than a step along the x-axis.  So in taking that step he should point his direction aligned with the y-axis than the x-axis.  That will produce a bigger increase per step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the direction of greatest ascent for a function,  $\\nabla f(x, y)$, is the direction which is a proportion of $\\frac{df}{dy}$ steps in the $y$ direction and $\\frac{df}{dx}$ in the $x$ direction.  So, for example, if $\\frac{df}{dy}$ = 5 and $\\frac{df}{dx}$ = 1, our next step she be five times more in the $y$ direction than the $x$ direction.  And this seems to be the path, more or less that our climbers are taking - some combination of $x$ and $y$, but tilted more towards the $y$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better understanding of a gradient, let's apply our understanding to a multivariable function.  Here is a plot of a function:\n",
    "\n",
    "$$f(x,y) = 2x + 3y $$\n",
    "\n",
    "![](./3dx3y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine being at the bottom left of the graph at the point $x = 1$, $y = 1$.  What would be the direction of steepest ascent?  It seems, just sizing it up visually, that we should move both in the positive $y$ direction and the positive $x$ direction.  Looking more carefully, it seems we should move **more** in the $y$ direction than the $x$ direction.  Let's see what our technique of taking the partial derivative indicates.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function $f(x,y)$, that is $ \\nabla f(x,y) = 2x + 3y $ is the following: \n",
    "\n",
    "$\\frac{df}{dx}(2x + 3y) = 2 $ and $\\frac{df}{dy}(2x + 3y) = 3 $.\n",
    "\n",
    "So what this tells us is to move in the direction of greatest ascent for the function $f(x,y) = 2x + 3y $, is to move up three and to the right two.  So we would expect our path of greatest ascent to look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ascent(x_values):\n",
    "    return list(map(lambda x: 1.5*x, x_values))\n",
    "\n",
    "zero_through_five = list(range(0, 6))\n",
    "y_values = ascent(zero_through_five)\n",
    "\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from graph import trace_values, plot\n",
    "init_notebook_mode(connected=True)\n",
    "# plot([trace_values(zero_through_five, y_values, mode = 'line')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./gradient-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./3dx3y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this path maps up well to what we see visually.  That is the idea behind gradient descent.  The gradient is the partial derivative with respect to each type of variable of a multivariable function, in this case $x$ and $y$.  And the import of the gradient is that it's direction is the direction of steepest ascent.  The negative gradient, that is the negative of each of the partial derivatives, is the direction of steepest descent.  So our direction of gradient descent for the graph above is $x = -2$, $y = -3$.  And looking at the two graphs above, it seems that the steepest downward direction is just the opposite of the steepest upward direction.  We get that by mathematically by simply taking the multiplying our partial derivatives by negative one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how we can use gradient descent to find the direction of steepest descent.  We saw that the direction of steepest descent is generally some combination of a change in our variables to produce the greatest negative rate of change.  \n",
    "\n",
    "We first how saw how to calculate the gradient **ascent**, or the gradient $\\nabla $, by calculating the partial derivative of a function with respect to the variables of the function.  So $\\nabla f(x, y) = \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta x} $.  This means that to take the path of greatest ascent, we should move $ \\frac{\\delta f}{\\delta y} $ divided by $ \\frac{\\delta f}{\\delta x} $.  So for example, when $ \\frac{\\delta f}{\\delta y}f(x, y)  = 3 $ , and $ \\frac{\\delta f}{\\delta x}f(x, y)  = 2$, we travelled in line with a slope of 3/2.\n",
    "\n",
    "For gradient descent, that is to find the direction of greatest decrease, we simply reverse the direction of our partial derivatives and move in $ - \\frac{\\delta f}{\\delta y}, - \\frac{\\delta f}{\\delta x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we learned the mathematical defintion of a gradient.  We saw that the gradient of a function was a combination of our partial derivatives with respect to each variable of that function.  We saw the direction of gradient descent was simply to move in the negative direction of the gradient.  So if the direction of ascent of a function is a move up and to the right, the descent is down and to the left. In this lesson we will apply gradient descent to our cost function to see how we can move towards a best fit regression line by changing variables of $m$ and $b$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Gradient Descent to our Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now that we know how to calculate gradient descent to find the direction of steepest descent we can apply this to our cost curve.  That is, we use gradient descent to calculate the change to make in our slope value and y-intercept value to produce the greatest rate of decrease in our cost.  This means that we can use gradient descent to take an initial regression regression line, and determine how to change our regression line next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./gradientdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, remember that for our cost function, we use the formula that $ RSS = \\sum(guess - actual)^2 = \\sum(\\overline{y} - y)^2 = \\sum(mx + b - y)^2$, for all $x$ values, where $mx + b $ represents our regression line.  Let's call our cost function $J$, and our error, RSS, is a function of our slope and our y-intercept, we represent this $J(m,b)$. So we say:\n",
    "\n",
    "$J(m, b) = \\sum(mx + b - y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so remember we want to find the values of $m$ and $b$ that minimize our RSS.  So we should have an idea of how to do that: use gradient descent to tell us how the direction of steepest descent to find our minimum.  So, as we know, to find the gradient of our function $J(m,b)$, we take the partial derivative with respect to each variable of the function, that is $\\frac{dJ}{dm}$ and $\\frac{dJ}{db}$.  In calculating the partial derivatives of our function $J(m,b)$, we won't change the result if we ignore the summation then replace it back at the end, so that's what we'll do to make our lives easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can take the partial derivative.   $\\frac{dJ}{dm}J(m, b) = \\frac{dJ}{dm}(mx + b - y)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is a tricky function to take the derivative of.  So we can use functional composition followed by the chain rule to make it easier.  Using functional composition, we can rewrite our function $J$ as two functions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$g(m,b) = mx +b - y$$\n",
    "$$J(g(m,b)) = (g(m,b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using the chain rule, to find the partial derivative with respect to a change in the slope we have: $$\\frac{dJ}{dm}J(g) = \\frac{dJ}{dg}J(g(m, b))*\\frac{dg}{dm}g(m,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now solving our derivatives individually we have: \n",
    "* $\\frac{dJ}{dg}J(g(m, b)) = \\frac{dJ}{dg}g(m,b)^2 = 2*g(m,b)$\n",
    "* $\\frac{dg}{dm}g(m,b) = \\frac{dg}{dm}mx +\\frac{dg}{dm}b - \\frac{dg}{dm}y = x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plugging these back into our chain rule we have: \n",
    "\n",
    "$\\frac{dJ}{dg}J(g(m,b))*\\frac{dg}{dm}g(m,b) = (2*g(m,b))*x = 2*(mx + b -y)*x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's calculate the partial derivative with respect to a change in the y-intercept we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dJ}{db}J(g) = \\frac{dJ}{dg}J(g)*\\frac{dg}{db}g(m,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we view our cost function as the same two functions $g(m,b)$ and $J(g(m,b))$.  From earlier, we know that $\\frac{dJ}{dg}J(g(m,b)) = \\frac{dJ}{dg}g(m,b)^2 = 2*g$.  The only thing left to calculate is $\\frac{dg}{db}g(m,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{dg}{db}g(m,b) = \\frac{dg}{db}(mx + b - y) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plug our terms into our chain rule and get: \n",
    "\n",
    "$$ \\frac{dJ}{dg}J(g)*\\frac{dg}{db}g(m,b) = 2*g(m,b)*1 = 2*(mx + b -y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have our two partial derivatives.  But as we know, to move point us in the direction of greatest descent we have to reverse the direction of each of these derivatives by reversing the sign, giving us:\n",
    "\n",
    "* $ \\frac{dJ}{dm}J(m,b) = -2*x(mx + b -y) $\n",
    "* $ \\frac{dJ}{db}J(m,b) = -2*(mx + b -y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as $mx + b$ = is just our regression line, we can simplify these formulas to be: \n",
    "\n",
    "* $ \\frac{dJ}{dm}J(m,b) = -2*x(\\overline{y} - y) $\n",
    "* $ \\frac{dJ}{db}J(m,b) = -2*(\\overline{y} - y) $\n",
    "\n",
    "and adding back in our summations we have: \n",
    "\n",
    "* $ \\frac{dJ}{dm}J(m,b) = -2*\\sum x(\\overline{y} - y) $\n",
    "* $ \\frac{dJ}{db}J(m,b) = -2*\\sum(\\overline{y} - y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is what what we'll do to find the our \"best fit regression line.\"  We'll start with an initial regression line with values of $m$ and $b$.  Then we'll go through our dataset, and with each point will use the above formulas to tell us how to update our regression line such that it continues to minimize our cost function.  We'll spend the next lesson we'll walking through this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section we developed some intuition for why the gradient of a function is the direction of steepest ascent and the negative gradient of a function is the direction of steepest decent.  Essentially, the gradient uses the partial derivatives to see what change will result from a of any of the function's dimensions, and then moves in that direction weights towards the partial derivative with the larger magnitude.\n",
    "\n",
    "We also practiced calculating some gradients, and ultimately calculated the gradient for our cost function.  This gave us two formulas which tell us how to update our regression line so that it descends along our cost function and approaches a \"best fit line\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
